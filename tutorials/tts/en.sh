#!/usr/bin/env bash
export OMP_NUM_THREADS=1
export HYDRA_FULL_ERROR=1
set -x

stage=$1
data_root=$DATA/en
# dataset=${data_root}/webex_speakers/en
# metadata=${dataset}/kareem.txt
# manifest=${dataset}/kareem.json
# manifest=tests/data/asr/an4_val.json
dataset=${data_root}/6097_5_mins
manifest_filepath=${dataset}/manifest.json
# generated by step -10
train_manifest=${dataset}/manifest.train.json
test_manifest=${dataset}/manifest.test.json
sup_data_folder=${dataset}/sup_data
whitelist=whitelists/lj_speech.tsv
config_path=conf
# fastpitch_config="fastpitch_align_22050"
fastpitch_config="fastpitch_align_v1.05.yaml"
wandb_project_name=$(basename ${dataset})
wandb_run_name=${fastpitch_config}
exp_dir=${wandb_project_name}
# additional files for phonemes, heteronyms & abbrv
helper_dir=/home/syl20/slgNM/tutorials/tts/tts_dataset_files
nemo_model=./tts_en_fastpitch_align.nemo

if [ ${stage} -eq 0 ]; then
    # manifest + normalization
    python ${dataset}/metadata2manifest.py --input ${metadata} --output ${manifest}
fi

if [ ${stage} -eq 10 ]; then
    /home/syl20/maui/scripts/datasets/train_test_split_file.py --train_size 0.98 ${manifest_filepath}
fi

if [ ${stage} -eq 1 ]; then
# only for DE
    python phonemize.py --train ${train} --test ${test} --val ${val} --lang 'de'
fi    

if [ ${stage} -eq 2 ]; then
    # extract pitch & pitch stats to stabilize training
    # not necessary if only fine-tuning
    # 1067 Pitch mean:117.12761688232422, Std: 23.40104103088379, Min:65.4063949584961, Max:1028.5239257812
    python extract_sup_data.py \
        --config-path ${config_path} \
        --config-name "ds_for_fastpitch_align.yaml" \
        manifest_filepath=${manifest_filepath} \
        sup_data_path=${sup_data_folder}
fi

pitch_mean=117.12761688232422
pitch_std=23.40104103088379
if [ ${stage} -eq 3 ]; then
    # train
    python fastpitch.py --config-path ${config_path} --config-name ${fastpitch_config} \
        model.train_ds.dataloader_params.batch_size=32 \
        model.validation_ds.dataloader_params.batch_size=32 \
        train_dataset=${dataset}/kareem.json \
        validation_datasets=${dataset}/kareem_val.json \
        sup_data_path=${sup_data_folder} \
        whitelist_path=${whitelist} \
        exp_manager.exp_dir=${exp_dir} \
        trainer.max_epochs=1000 \
        pitch_mean=${pitch_mean} \
        pitch_std=${pitch_std} \
        +exp_manager.create_wandb_logger=true \
        +exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \
        +exp_manager.wandb_logger_kwargs.project=${wandb_project_name}
fi

if [ ${stage} -eq 4 ]; then
    pitch_mean=117.12761688232422
    pitch_std=23.40104103088379
    pitch_fmin=30
    pitch_fmax=512
    n_speakers=1
    batch_size=48 #24
    python fastpitch_finetune.py --config-name=${fastpitch_config} \
        train_dataset=${train_manifest} \
        validation_datasets=${test_manifest} \
        sup_data_path=${sup_data_folder} \
        phoneme_dict_path=${helper_dir}/cmudict-0.7b_nv22.01 \
        heteronyms_path=${helper_dir}/heteronyms-030921 \
        whitelist_path=${helper_dir}/lj_speech.tsv \
        exp_manager.exp_dir=${exp_dir}_finetune \
        +init_from_nemo_model=${nemo_model} \
        +trainer.max_steps=1000 ~trainer.max_epochs \
        trainer.check_val_every_n_epoch=25 \
        model.train_ds.dataloader_params.batch_size=${batch_size} model.validation_ds.dataloader_params.batch_size=24 \
        model.n_speakers=${n_speakers} model.pitch_mean=${pitch_mean} model.pitch_std=${pitch_std} \
        model.pitch_fmin=${pitch_fmin} model.pitch_fmax=${pitch_fmax} model.optim.lr=2e-4 \
        ~model.optim.sched model.optim.name=adam trainer.devices=8 trainer.strategy=ddp \
        +model.text_tokenizer.add_blank_at=true \
        +exp_manager.create_wandb_logger=true \
        +exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \
        +exp_manager.wandb_logger_kwargs.project=${wandb_project_name}_finetune
fi

if [ ${stage} -eq 5 ]; then
    python fastpitch_synth.py \
        --specgen_ckpt_dir ${exp_dir}_finetune \
        --vocoder "tts_hifigan" \
        --input_text "This is a test of how well this new model is behaving." \
        --output_audio "this_is_a_test.wav" \
        --sr 22050
fi

if [ ${stage} -eq 6 ]; then
    # hifigan data prep for train & validation sets
    mels_dir=${sup_data_folder}/mels
    manifest_path=${train_manifest}
    for manifest in ${train_manifest} ${test_manifest}; do
        # echo $(basename ${manifest%.*}).hifigan.json
        python hifigan_dataprep.py \
            --manifest_path ${manifest} \
            --mels_dir ${mels_dir} \
            --specgen_ckpt_dir ${exp_dir}_finetune \
            --hifigan_manifest_path ${dataset}/$(basename ${manifest%.*}).hifigan.json
    done
fi

if [ ${stage} -eq 7 ]; then
    # hifigan fine-tuning
    train=${dataset}/manifest.train.hifigan.json
    val=${dataset}/manifest.test.hifigan.json
    pretrained_checkpoint="tts_hifigan" # pre-trained nvidia model

    python hifigan_finetune.py \
        --config-name=hifigan.yaml \
        model.train_ds.dataloader_params.batch_size=32 \
        model.max_steps=1000 \
        model.optim.lr=0.00001 \
        ~model.optim.sched \
        train_dataset=${train} \
        validation_datasets=${val} \
        exp_manager.exp_dir=test_finetune_hifigan \
        +init_from_pretrained_model=${pretrained_checkpoint} \
        trainer.check_val_every_n_epoch=10 \
        trainer.devices=8 \
        model/train_ds=train_ds_finetune \
        model/validation_ds=val_ds_finetune
        # +exp_manager.create_wandb_logger=true \
        # +exp_manager.wandb_logger_kwargs.name=${wandb_run_name} \
        # +exp_manager.wandb_logger_kwargs.project=${wandb_project_name}_finetune_hifigan
fi